\documentclass[12pt]{article}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{url} % not crucial - just used below for the URL

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{1}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{-.3in}%
\addtolength{\topmargin}{-.8in}%

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{xfrac}
\usepackage{hyperref}
\usepackage{color}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{caption}
\usepackage{subcaption}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\sstar}{s^*}
\newcommand{\sfunc}{s}

\numberwithin{equation}{section}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}

\newcommand{\glnote}[1]{\textcolor{red}{[GL: #1]}}

\begin{document}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if1\blind
{
  \title{\bf Approximating Likelihood Ratios with Calibrated Discriminative Classifiers}
  \author{Kyle Cranmer, Juan Pavez and Gilles Louppe\\
          New York University}
  \maketitle
} \fi

\if0\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Title}
\end{center}
  \medskip
} \fi

\bigskip
\begin{abstract}

% In particle physics likelihood ratio tests are established tools for statistical
% inference.  These tests are complicated by the fact that computer simulators are
% used as a generative model for the data, but they do not provide a way to
% evaluate the likelihood function. We demonstrate how discriminative classifiers
% can be used to approximate the likelihood function when a generative model for
% the data is available for training and calibration.  This offers an approach to
% parametric inference when simulators are used that is complementary to
% approximate Bayesian computation.

In many fields of science, generalized likelihood ratio tests are established tools for
statistical inference. In practice, these tests are often complicated by the fact
that computer simulators are used as a generative model for the data, which does
not provide a way to evaluate the likelihood function. In this paper, we
demonstrate that likelihood ratios are invariant under dimensionality reductions $\mathbb{R}^p \mapsto \mathbb{R}$,
provided the transformation is itself monotonic with the likelihood ratio. As a direct
consequence, we show that discriminative classifiers can be used to
approximate the generalized likelihood ratio statistic when only a generative model for the data is
available. In particular, the proposed method offers a machine learning-based
approach to statistical inference that is complementary to likelihood-free
Bayesian inference algorithms, such as Approximate Bayesian Computation, as it does
not require the definition of a prior over model parameters. Experimental results
on artificial problems illustrate the potential of the proposed method.

\end{abstract}

\noindent%
{\it Keywords:}  likelihood ratio, likelihood-free inference, classification, particle physics
\vfill

\newpage
\spacingset{1.45} % DON'T change the spacing!


% Introduction =================================================================

% - Introduction section, mostly the same as before:
%    * likelihood free setup
%    * searches for particles at LHC as an example
%    * brief and clear description of the contribution of this paper
%      (i.e., density ratio estimation with calibrated classifiers,
%             as an alternative to other methods for parameter inference)
% - outline

\section{Introduction}
\label{sec:introduction}

% The likelihood function is the central object that summarizes the information
% from an experiment needed for inference of model parameters. The likelihood
% function is key to Bayesian inference, and many areas of science that report the
% results of classical hypothesis tests or confidence intervals use the
% (generalized or profile) likelihood ratio as a test statistic. It is
% increasingly common that a simulator (or generative model) is used to describe
% complex processes that tie parameters $\theta$ of an underlying theory and
% measurement apparatus to high-dimensional observations $x$. Directly evaluating
% the likelihood ratio in these cases is often impossible or is computationally
% impractical. Approximate Bayesian Computation (ABC) is one approach to parameter
% inference in this simulation-based or likelihood-free
% setting~\citep{Rubin1984,Tavare1997,Marin2011}. Here we consider an alternative
% approach that can also be used in a classical setting where a prior over the
% parameters is not available. In particular, we demonstrate how discriminative
% classifiers can be used to construct equivalent likelihood ratio tests when a
% generative model for the data is available for training and calibration.

The likelihood function is the central object that summarizes the information
from an experiment needed for inference of model parameters. It
is key to many areas of science that report the results of classical
hypothesis tests or confidence intervals using the (generalized or profile)
likelihood ratio as a test statistic. At the same time, with the advance of
computing technology, it has become increasingly common that a simulator (or
generative model) is used to describe complex processes that tie parameters
$\theta$ of an underlying theory and measurement apparatus to high-dimensional
observations $\mathbf{x}$. However, directly evaluating the likelihood function
in these cases is often impossible or is computationally impractical.

The main result of this paper is to show that the likelihood ratio is invariant
under dimensionality reductions $\mathbb{R}^p \mapsto \mathbb{R}$, under the assumption that the corresponding
transformation is itself monotonic with the likelihood ratio. As a direct consequence,
we derive and propose an alternative machine learning-based approach for
likelihood-free inference that can also be used in a classical (frequentist)
setting where a prior over the model parameters is not available. More
specifically, we demonstrate that discriminative classifiers can be used
to construct equivalent generalized likelihood ratio test statistics when only a generative model for
the data is available for training and calibration.

% As a concrete example, consider searches for new particles at the Large Hadron
% Collider. The simulator that is sampling from $p(x|\theta)$ is based on quantum
% field theory, a detailed simulation of the particle detector, and data
% processing algorithms that transform raw sensor data into the feature vector
% $x$~\citep{Sjostrand:2006za,Agostinelli:2002hh}.
%
% The ATLAS and CMS experiments have published  hundreds papers where the final
% result was formulated as a hypothesis test or confidence interval using a
% generalized  likelihood ratio test~\citep{Cowan:2010js}. This includes the
% discovery of the Higgs boson~\citep{Aad:2012tfa,Chatrchyan:2012ufa} and
% subsequent measurement of its properties.
%
% The bulk of the likelihood ratio tests at the LHC are based on the distribution
% of a single event-level feature that discriminates between a hypothesized
% process of interest (labeled \textit{signal}) and various other processes
% (labeled \textit{background}). Typically,  pseudo-data from the simulator are
% used to approximate the density at various parameter points, and an
% interpolation algorithm is used to approximate the parametrized
% model~\citep{Cranmer:2012sba}.
%
% To improve the statistical power of these tests, hundreds of these searches have
% utilized supervised learning to train discriminative classifiers that take
% commonly used. Recently, there has been progress in using deep
% networks~\citep{Baldi:2014kfa} and a NIPS workshop synthesizing the lessons
% learned during HiggsML~\citep{HepML}, the largest Kaggle challenge in history.

As a concrete example, let us consider searches for new particles at the Large
Hadron Collider (LHC). The simulator that is sampling from $p(\mathbf{x}|\theta)$ is
based on quantum field theory, a detailed simulation of the particle detector,
and data processing algorithms that transform raw sensor data into the feature
vector $\mathbf{x}$~\citep{Sjostrand:2006za,Agostinelli:2002hh}. The ATLAS and CMS
experiments have published  hundreds of papers where the final result was
formulated as a hypothesis test or confidence interval using a generalized
likelihood ratio test~\citep{Cowan:2010js}, including most notably the discovery
of the Higgs boson~\citep{Aad:2012tfa,Chatrchyan:2012ufa} and subsequent
measurement of its properties. The bulk of the likelihood ratio tests at the LHC
are based on the distribution of a single event-level feature that discriminates
between a hypothesized process of interest (labeled \textit{signal}) and various
other processes (labeled \textit{background}). Typically, data generated from the
simulator are used to approximate the density at various parameter points, and
an interpolation algorithm is used to approximate the parameterized
model~\citep{Cranmer:2012sba}. In particular, to improve the statistical power
of these tests, hundreds of these searches have already been using supervised
learning to train discriminative classifiers that take advantage of a high
dimensional feature vector $\mathbf{x}$.

% While classification accuracy can lead to optimal approaches for simple
% hypothesis tests~\citep{Dempster1965}, that is no longer true in the context of
% parameter estimation or composite hypothesis tests with nuisance parameters. As
% noted in ~\citep{Whiteson:2006ws}, ``such methods are suboptimal because they
% assume that the selector with the highest classification accuracy will yield a
% mass measurement with the smallest statistical uncertainty.'' The key
% distinction is that evaluating the loss for classification is composed of many
% per-event operations, while evaluating the loss for a mass measurement (e.g. the
% variance of an estimator for the mass parameter) is a per-experiment operation
% involving a data set with many events. They went on to demonstrate a
% computationally intensive stochastic optimization technique based on the
% per-experiment loss out performed the two stage selection-estimation process.
%
% The initial motivation for this work was to extend the typical usage of
% discriminative classifiers in HEP to be robust to nuisance parameters in the
% simulators. The scope of the result expanded once it became clear that this
% offers a way to approximate the likelihood function $p(x|\theta)$ in what is
% typically considered the likelihood-free setting. This approach is complementary
% to ABC as it does not require a prior over the parameters and can also be used
% in the classical (frequentist) setting. A strength of this approach is that it
% separates the quality of the approximation of the target likelihood from the
% quality of the calibration.  In Section~\ref{S:Related} we discuss the scheme
% sketched by \cite{Neal:2007zz} that also suggests using a classifier  as a
% dimensionality reduction map to aid in the estimation of the likelihood
% function.

The rest of the paper is organized as follows. In
Section~\ref{sec:likelihood-ratio-tests}, we first introduce the likelihood
ratio test statistic in the setting of simple hypothesis testing, and then
outline how it can be computed exactly using calibrated classifiers.
In Section~\ref{sec:generalized-likelihood-ratio}, we generalize the proposed
approach to the case of composite hypothesis testing and discuss directions for
approximating the statistic efficiently. We then illustrate the proposed
method in Section~\ref{sec:examples} and outline how it could improve
statistical analysis within the field of high energy physics. Related work
and conclusions are finally presented in sections~\ref{sec:related} and \ref{sec:conclusions}.



% Likelihood ratio tests =======================================================

% - Introduce the mathematical problem with notations (was 1.1 and 1.2 before)
% - I would keep some of the comments of 1.3, but I feel some are not necessary

\section{Likelihood ratio tests}
\label{sec:likelihood-ratio-tests}

\subsection{Simple hypothesis testing}

% \subsection{Notation and Assumptions}
%
% We use the following notation:
% \begin{itemize}
%  \item $x$: a vector of features for an event
%  \item $D$: a data set of $D=\{x_1, \dots, x_n\}$, where $x_e$ are assumed to be i.i.d.
%  \item $\theta$: parameters of a statistical model
% \item $p(x| \theta)$:  probability density  (simulation-based model) for $x$ given $\theta$
% %\item $s(x)$: real-valued score from a machine learning classification algorithm (or any map $s: X\to\mathbb{R}$)
% \item $y$: a class label used for training a classifier.
% \item $s(x;\theta_0, \theta_1)$: real-valued discriminative classification score, parametrized by $\theta_0$ and $\theta_1$
% %\item $p( s | \theta )$ The probability density function for $s$ implied by $p(x|\theta)$ and $s(x)$
% \item $p( s_{\theta_0, \theta_1} | \theta )$: The probability density  for $s(x; \theta_0, \theta_1)$ implied by $p(x|\theta)$
% \end{itemize}
% We will assume the $x_e$ are i.i.d., so that $p(D|\theta) = \prod_{e=1}^n p(x_e | \theta)$.

% \subsection{Prelude}
%
% In the setting where one is interested in simple hypothesis testing between a null $\theta=\theta_0$ against an alternate $\theta=\theta_1$, the Neyman-Pearson lemma states that the likelihood ratio
% \begin{equation}
% T(D; \theta_0, \theta_1) = \prod_{e=1}^n \frac{ p(x_e|\theta_0)}{ p(x_e|\theta_1)}
% \end{equation}
% is the most powerful test statistic. In order to evaluate $T(D)$, one must be able to evaluate the probability density
% $p(x| \theta)$ at any value $x$. However, it is increasingly common in science that one has a complex simulation that
% can act as generative model  for $p(x|\theta)$, but one cannot evaluate the density directly. For instance, this is the case
% high energy physics where the simulation of particle detectors can only be done in the `forward mode'. This same setting has been considered by \cite{ClaytonScott}, \cite{JMLR:v14:tong13a}, and \cite{Neal:2007zz}.

Let $\mathbf{X}$ be a random vector with values $\mathbf{x} \in {\cal X}
\subseteq \mathbb{R}^p$ and let $p_\mathbf{X}(\mathbf{x}|\theta)$ denote the
density probability of $\mathbf{X}$ at value $\mathbf{x}$ under the
parameterization $\theta$. Let also assume i.i.d. observed data ${\cal D} = \{
\mathbf{x}_1, \dots, \mathbf{x}_n \}$. In the setting where one is interested in
simple hypothesis testing between a null $\theta=\theta_0$ against an alternate
$\theta=\theta_1$, the Neyman-Pearson lemma states that the likelihood ratio
\begin{equation}\label{eqn:likelihood-ratio-test}
\lambda({\cal D}; \theta_0, \theta_1) = \prod_{\mathbf{x} \in {\cal D}} \frac{ p_\mathbf{X}(\mathbf{x}|\theta_0)}{ p_\mathbf{X}(\mathbf{x}|\theta_1)}
\end{equation}
is the most powerful test statistic.

In order to evaluate $\lambda({\cal D})$, one must be able to evaluate the probability
densities $p_\mathbf{X}(\mathbf{x}| \theta_0)$ and $p_\mathbf{X}(\mathbf{x}| \theta_1)$ at any value $\mathbf{x}$. However,
it is increasingly common in science that one has a complex simulation that can
act as generative model for $p_\mathbf{X}(\mathbf{x}|\theta)$, but one cannot
evaluate the density directly. For instance, this is the case in high energy
physics~\citep{Neal:2007zz} where the simulation of particle detectors can only
be done in the forward mode.


% Approximating likelihood ratios with classifiers =============================

% - Core of the paper (was section 2 + my note on slack)
% - Derive theorem for the ratio of transformed densitites
% - Show that consistent regressors minimizing the squared error loss fulfill conditions of the theorem
% - Discuss that training on signal vs background instead of signal+background vs background is OK because this learns a function which is 1:1. => focus the capacity of the classifier; Generalization to decomposed ratios
% - Discuss about the need of calibration in case of imperfect classifiers (was first paragraphs of section 5)

\subsection{Approximating likelihood ratios with classifiers}
\label{sec:approx}

% The main result of this paper is to generalize the observation that one can form an equivalent test based on
% %\begin{equation}
% %T'(D) = \prod_{e=1}^n \frac{ p(\,s(x_e; \theta_1, \theta_0) \mid \theta_1)}{ p(\,s(x_e; \theta_1, \theta_0)\mid\theta_0)}
% %\end{equation}
% \begin{equation}\label{eq:equivLRtest}
% T'(D; \theta_0, \theta_1) = \prod_{e=1}^n \frac{ p(s_e | \theta_0)}{ p(s_e | \theta_1)}
% \end{equation}
% if
% \begin{equation}\label{eq:montonic}
% s_e = s(x_e; \theta_0, \theta_1) = m\left(\, p(x_e|\theta_0) / p(x_e|\theta_1) \,\right) \;
% %s_e = s(x_e; \theta_0, \theta_1) = m\left(\frac{ p(x_e|\theta_0)}{ p(x_e|\theta_1)} \right) \;
% \end{equation}
% where $m$ is any strictly increasing or decreasing function. This result will be proven below.
% This allows us to recast the original likelihood ratio test into an alternate form in which supervised learning is used to train the discriminative classifier $s(x; \theta_0, \theta_1)$. The discriminative classifier can be trained with data $(x,u=0)$ generated
% from $p(x|\theta_0)$ and $(x,u=1)$ generated from $p(x|\theta_1)$. In Section~\ref{S:GLR} we extend this result to generalized likelihood ratio tests, where it will be useful to have the classifier  parametrized in terms of $(\theta_0, \theta_1)$.
%
% Here we see that the original goal for simple hypothesis testing (i.e. to make a decision to accept or reject the null hypothesis based on the entire data set $D$) has been reformulated into a per-event classification problem. This follows from the fact that we assume the $x_e$ to be i.i.d.

The main result of this paper is to
generalize the observation that one can form a test statistic
\begin{equation}\label{eqn:likelihood-ratio-test-equiv}
\lambda'({\cal D}; \theta_0, \theta_1) = \prod_{\mathbf{x} \in {\cal D}} \frac{ p_\mathbf{U}(u=s(\mathbf{x}) | \theta_0)}{ p_\mathbf{U}(u=s(\mathbf{x}) | \theta_1)}
\end{equation}
that is strictly equivalent to \ref{eqn:likelihood-ratio-test}, provided the change
of variable $\mathbf{U} = s(\mathbf{X})$ is based
on a (parameterized) function $s$ that is strictly monotonic with the density ratio
\begin{equation}
r(\mathbf{x};\theta_0, \theta_1) = \frac{p_\mathbf{X}(\mathbf{x}|\theta_0)}{p_\mathbf{X}(\mathbf{x}|\theta_1)}.
\end{equation}
As derived below, this allows to recast the original likelihood ratio test into
an alternate form in which supervised learning can be used to build
$s(\mathbf{x})$ as a discriminative classifier.  In
Section~\ref{sec:generalized-likelihood-ratio} we extend this result to
generalized likelihood ratio tests, where it will be useful to have the
classifier decision function $s$ parameterized in terms of $(\theta_0, \theta_1)$.

%\subsection{Likelihood ratios under change of variables}

% \section{Dimensionality reduction and calibration}
%
% We are interested in reformulating the target likelihood ratio
% \begin{equation}
% \ln T(D; \theta_0, \theta_1) =   \sum_{e=1}^n \underbrace{\log \left[ \frac {p(x_e | \theta_0) }{ p(x_e | \theta_1) } \right]}_{q(x_e)} \;.
% \end{equation}
% Here we see that the test statistic $T$ for the experiment is composed of a sum over events of the per-event function $q(x)$. A sum over a monotonic, but non-linear function of $q(x)$ would not lead to an equivalent statistic.
%
% The important part of the per-event function $q(x)$ is that it defines iso-contours in the feature space $x$. As we will show, our goal is to learn a monotonic function of $p(x|\theta_0)/p(x|\theta_1)$, which will share the same iso-contours. Then the remaining challenge is to find the appropriate monotonic function that gives back a linear function of $q(x)$. Our claim is that the generative model $p(x|\theta)$ can be used to calibrate the density $p(s|\theta)$ and that
% \begin{equation}\label{eq:enveloping}
% \ln T'(D; \theta_0, \theta_1) = \sum_{e=1}^n \underbrace{\log \left[ \frac {p(s_e | \theta_0) }{ p(s_e | \theta_1) } \right]}_{q(s_e)} \;,
% \end{equation}
% leads to an equivalent statistic.
%
% For notational simplicity, let $p_0(x) = p(x|\theta_0)$, $p_1(x) = p(x|\theta_1)$, and $\sfunc(x)=s(x; \theta_0, \theta_1)$.
% The distribution of $x$ totally determines the distribution of $s$.
% In the application at hand, the function $s$ maps a high-dimensional feature vector $x$ to $\mathbb{R}^+$.
% Let $\Omega_{\sstar}$ be the level set $\{x \mid s(x; \theta_0, \theta_1) = \sstar \}$ and \mbox{$\hat{n}=\nabla s(x) / |\nabla s(x)|$} be the orthonormal vector to $\Omega_{\sstar}$ at the point $x$.
%
% We need to show that for all $x$, the density
% \begin{equation}
% p(q_x|\theta) = \int dx \delta(q_x-q_x(x)) p(x|\theta)  / | \hat{n} \cdot \nabla q_x  |
% \end{equation}
% is equal to the density
% \begin{equation}
% p(q_s|\theta) = \int dx \delta(q_s-q_s(s(x))) \, p(x|\theta) \, / | \hat{n} \cdot \nabla q_s  | \; .
% \end{equation}
% It is sufficient to show that $q_x(x) = q_s(s(x))$.
% %$ \forall x\in\Omega_{\sstar}$.
% The function $q_s(s)$ is based on the induced densities $p_0(s)$ and $p_1(s)$.  The induced density $p_1(s)$ is given by
% \begin{equation}
% p_1(\sstar) = \int dx \delta(\sstar-s(x)) p_1(x) = \int d\Omega_{\sstar} p_1(x)  / | \hat{n} \cdot \nabla s  |
% \end{equation}
% and a similar equation for $p_0(s)$.
% %\textbf{Do we need Jacobian for x $\to$ s independent of delta function part, I think that's double counting?}
%
% \textbf{\flushleft Theorem 1:}
% We have the following equality
% \begin{equation}
% \frac{p_1(s(x))}{p_0(s(x))} =  \frac{p_1(x)}{p_0(x)}  \; . %\;\hspace{3em} \forall x\in\Omega_{\sstar}\; .
% \end{equation}
% \textbf{Proof}
% For $x\in \Omega_{\sstar}$, we can factor out of the integral the constant $p_1(x)/p_0(x)$.
% Thus
% \begin{equation}
% p_1(\sstar) =  \int d\Omega_{\sstar} p_1(x) / | \hat{n} \cdot \nabla s  |= \frac{p_1(x)}{p_0(x)} \int d\Omega_{\sstar} p_0(x)  / | \hat{n} \cdot \nabla s  | \;,
% %p_1(\sstar) = \int dx \delta(\sstar-s(x)) p_1(x) = \int d\Omega_{\sstar} p_1(x) / | \hat{n} \cdot \nabla s  |= \frac{p_1(x)}{p_0(x)} \int d\Omega_{\sstar} p_0(x)  / | \hat{n} \cdot \nabla s  | \;,
% \end{equation}
% and the integrals cancel in the likelihood ratio
% \begin{equation}
% \frac{p_1(\sstar)}{p_0(\sstar)} = \frac{p_1(x)}{p_0(x)} \frac{\int d\Omega_{\sstar} p_0(x)/ | \hat{n} \cdot \nabla s  |}{ \int d\Omega_{\sstar} p_0(x) / | \hat{n} \cdot \nabla s  |} = \frac{p_1(x)}{p_0(x)}  \;\hspace{3em} \forall x\in\Omega_{\sstar}.
% \end{equation}
%
% One can think of the ratio $p_1(s)/p_0(s)$ as a way of calibrating the the discriminative classifier and correcting for the monotonic transformation $m$ of the desired likelihood ratio as in Eq.~\ref{eq:montonic}.

\begin{theorem}
    \label{thm:ratio-equivalence}
    Let $\mathbf{X}$ be a random vector vector with values in ${\cal X} \subseteq \mathbb{R}^p$ and parameterized probability
    density $p_{\mathbf{X}}(\mathbf{x} = (x_1, ..., x_p)|\theta)$ and let
    $s : \mathbb{R}^p \mapsto \mathbb{R}$ be a function monotonic with the density ratio
    $r(\mathbf{x};\theta_0,\theta_1) = \frac{p_\mathbf{X}(\mathbf{x}|\theta_0)}{p_\mathbf{X}(\mathbf{x}|\theta_1)}$,
    for given parameters $\theta_0$ and $\theta_1$. In these conditions,
    \begin{equation}
        r(\mathbf{x};\theta_0,\theta_1) = \frac{p_\mathbf{X}(\mathbf{x}|\theta_0)}{p_\mathbf{X}(\mathbf{x}|\theta_1)} = \frac{p_\mathbf{U}(u=s(\mathbf{x})|\theta_0)}{p_\mathbf{U}(u=s(\mathbf{x})|\theta_1)},
    \end{equation}
    where $p_\mathbf{U}(u=s(\mathbf{x};\theta_0,\theta_1)|\theta)$ is the induced probability density of
    $\mathbf{U} = s(\mathbf{X};\theta_0,\theta_1)$.
\end{theorem}

\begin{proof}
Starting from the definition of the probability density function, we have
\begin{align}
% p_{\mathbf{U}}(u=s(\mathbf{x})|\theta_0) &= \frac{d}{dy} P(s(\mathbf{X}) \leq y) \nonumber \\
p_{\mathbf{U}}(u=s(\mathbf{x})|\theta_0) &= \frac{d}{du}  \int_{\{\mathbf{x}^\prime \in \mathbb{R}^p : s(\mathbf{x}^\prime) \leq u\}} p_\mathbf{X}(\mathbf{x}^\prime|\theta_0) d\mathbf{x}^\prime \nonumber \\
% &= \frac{d}{dy} \int_{\mathbb{R}^p} H(y - s(\mathbf{x}^\prime)) p_\mathbf{X}(\mathbf{x}^\prime|\theta_0) d\mathbf{x}^\prime \nonumber \\
% &= \int_{\mathbb{R}^p} \frac{d}{dy} H(y - s(\mathbf{x}^\prime)) p_\mathbf{X}(\mathbf{x}^\prime|\theta_0) d\mathbf{x}^\prime \nonumber \\
&= \int_{\mathbb{R}^p} \delta(u - s(\mathbf{x}^\prime)) p_\mathbf{X}(\mathbf{x}^\prime|\theta_0) d\mathbf{x}^\prime
\end{align}
Intuitively, this expression can be understood as the integral
over all $\mathbf{x}^\prime \in \mathbb{R}^p$ such that $s(\mathbf{x}^\prime) = u$, as picked
by the Dirac $\delta$ function. Given Theorem 6.1.5 of \citet{Hrmander1990},
it further comes
\begin{equation}
p_{\mathbf{U}}(u=s(\mathbf{x})|\theta_0) = \int_{\{\mathbf{x}^\prime \in \mathbb{R}^p : s(\mathbf{x}^\prime) = u\}} \frac{1}{|\nabla s(\mathbf{x}^\prime)|} p_\mathbf{X}(\mathbf{x}^\prime|\theta_0) dS_{\mathbf{x}^\prime} \label{eqn:hormander}
\end{equation}
where $|\nabla s(\mathbf{x}^\prime)| = \sqrt{\sum_{i=1}^p |\frac{\partial}{\partial x_i} s(\mathbf{x}^\prime)|^2}$
and where $dS_{\mathbf{x}^\prime}$ is the Euclidean surface measure on $\{\mathbf{x}^\prime \in \mathbb{R}^p : s(\mathbf{x}^\prime) = u\}$.
Also, since $s(\mathbf{x})$ is monotonic with
$\frac{p_\mathbf{X}(\mathbf{x}|\theta_0)}{p_\mathbf{X}(\mathbf{x}|\theta_1)}$,
it exists an invertible function $m:\mathbb{R}^+ \mapsto \mathbb{R}$ such
that $s(\mathbf{x}) = m(\frac{p_\mathbf{X}(\mathbf{x}|\theta_0)}{p_\mathbf{X}(\mathbf{x}|\theta_1)})$.
In particular, we have
\begin{align}
\frac{p_\mathbf{X}(\mathbf{x}|\theta_0)}{p_\mathbf{X}(\mathbf{x}|\theta_1)} &= m^{-1}(s(\mathbf{x})) \nonumber \\
p_\mathbf{X}(\mathbf{x}|\theta_0) &= m^{-1}(s(\mathbf{x})) p_\mathbf{X}(\mathbf{x}|\theta_1) \label{eqn:mapping}
\end{align}
Combining equations \ref{eqn:hormander} and \ref{eqn:mapping}, the density ratio $\frac{p_\mathbf{X}(\mathbf{x}|\theta_0)}{p_\mathbf{X}(\mathbf{x}|\theta_1)}$ can be pulled out of the integral, resulting in
\begin{align}
p_{\mathbf{U}}(u=s(\mathbf{x})|\theta_0) &= \int_{\{\mathbf{x}^\prime \in \mathbb{R}^p : s(\mathbf{x}^\prime) = u\}}  \frac{1}{|\nabla s(\mathbf{x}^\prime)|} m^{-1}(s(\mathbf{x}^\prime)) p_\mathbf{X}(\mathbf{x}^\prime|\theta_1) dS_{\mathbf{x}^\prime} \nonumber \\
&= \int_{\{\mathbf{x}^\prime \in \mathbb{R}^p : s(\mathbf{x}^\prime) = u\}}  \frac{1}{|\nabla s(\mathbf{x}^\prime)|} m^{-1}(u) p_\mathbf{X}(\mathbf{x}^\prime|\theta_1) dS_{\mathbf{x}^\prime} \nonumber \\
&= m^{-1}(s(\mathbf{x})) \int_{\{\mathbf{x}^\prime \in \mathbb{R}^p : s(\mathbf{x}^\prime) = u\}}  \frac{1}{|\nabla s(\mathbf{x}^\prime)|}  p_\mathbf{X}(\mathbf{x}^\prime|\theta_1) dS_{\mathbf{x}^\prime} \nonumber \\
&= \frac{p_\mathbf{X}(\mathbf{x}|\theta_0)}{p_\mathbf{X}(\mathbf{x}|\theta_1)} \int_{\{\mathbf{x}^\prime \in \mathbb{R}^p : s(\mathbf{x}^\prime) = u\}}  \frac{1}{|\nabla s(\mathbf{x}^\prime)|}  p_\mathbf{X}(\mathbf{x}^\prime|\theta_1) dS_{\mathbf{x}^\prime}. \label{eqn:factorization}
\end{align}
Similarly, Equation~\ref{eqn:hormander} can be used to derive $p_{\mathbf{U}}(u=s(\mathbf{x})|\theta_1)$, finally yielding
\begin{align}
\frac{p_{\mathbf{U}}(u=s(\mathbf{x})|\theta_0)}{p_{\mathbf{U}}(u=s(\mathbf{x})|\theta_1)} &= \frac{p_\mathbf{X}(\mathbf{x}|\theta_0)}{p_\mathbf{X}(\mathbf{x}|\theta_1)} \frac{\int_{\{\mathbf{x}^\prime \in \mathbb{R}^p : s(\mathbf{x}^\prime) = u\}}  \frac{1}{|\nabla s(\mathbf{x}^\prime)|}  p_\mathbf{X}(\mathbf{x}^\prime|\theta_1) dS_{\mathbf{x}^\prime}}{ \int_{\{\mathbf{x}^\prime \in \mathbb{R}^p : s(\mathbf{x}^\prime) = u\}}  \frac{1}{|\nabla s(\mathbf{x}^\prime)|}  p_\mathbf{X}(\mathbf{x}^\prime|\theta_1) dS_{\mathbf{x}^\prime} } \nonumber \\
&= \frac{p_\mathbf{X}(\mathbf{x}|\theta_0)}{p_\mathbf{X}(\mathbf{x}|\theta_1)}. \label{eqn:jacob}
\end{align}
\end{proof}

% \subsection{Probabilistic classification for likelihood ratios}
% \label{sec:clf-for-ratios}

% \subsection{The fixed discriminative classification setting} For fixed
% $\theta_0$ and $\theta_1$ we can generate large samples from each model and
% train a classifier. To be concrete, let us use $p(x|\theta_0)$ to generate
% training data $(x_i,  y_i=0)$ and $p(x|\theta_1)$ to generate training data
% $(x_i , y_i=1)$. With balanced training data
% \mbox{($p(u=1)=p(u=0)=\sfrac{1}{2}$)} a quadratic loss function will lead to
% classifiers that approximate the regression function  $\hat{s}(x) \approx p(y|x) =
% p(x|\theta_1)/(p(x|\theta_0)+p(x|\theta_1))$, which is  monotonic with the
% desired per-event likelihood ratio $q(x)$. Thus, standard supervised learning
% algorithms with various surrogate loss functions lead to discriminative
% classifiers that approximate a monotonic function of per-event likelihood ratio
% $q(x)$.

In light of this result, the likelihood ratio estimation problem can now
be recast as a (probabilistic) classification problem, by noticing that the decision
function
\begin{equation}\label{eqn:best-s-clf}
s^*(\mathbf{x}) = \frac{p_{\mathbf{X}}(\mathbf{x}|\theta_1)}{p_{\mathbf{X}}(\mathbf{x} | \theta_0) + p_{\mathbf{X}}(\mathbf{x} | \theta_1)}.
\end{equation}
modeled by a classifier trained to distinguish samples $\mathbf{x} \sim p_{\theta_0}$
from samples $\mathbf{x} \sim p_{\theta_1}$ satisfies conditions of
Theorem~\ref{thm:ratio-equivalence} (see Appendix~\ref{app:clf-for-s} for further details).
In other words, supervised learning yields a sufficient
procedure for Theorem~\ref{thm:ratio-equivalence} to hold, guaranteeing that any
{\it universally strongly consistent} algorithm can be used for learning $s^*$.
Note however, that it is not a necessary procedure since
Theorem~\ref{thm:ratio-equivalence} holds for any monotonic function $m$ of the
density ratio, i.e., not for $m(r(\mathbf{x})) = \frac{1}{1 +
r(\mathbf{x})}$ only.
Equivalently,
Theorem~\ref{thm:ratio-equivalence} shows that in case we learn a probabilistic
classifier $s(\mathbf{x})$ which is imperfect up to a monotonic transformation
of $r(\mathbf{x})$, then one can still resort to calibration (i.e., modeling
$p_{\mathbf{U}}(u=s(\mathbf{x}))$) to compute $r(\mathbf{x})$ exactly.


\subsection{Learning and calibrating $s$}

In order for the proposed approach to be useful in the likelihood-free setting,
we need to be able to approximate both $s(\mathbf{x})$ and
$p(s(\mathbf{x})|\theta)$ based on a finite number of samples $\{\mathbf{x}_i\}$
drawn from the generative model $p(\mathbf{x}|\theta)$.

As outlined above, any consistent probabilistic classification algorithm can be
used for learning an approximate  map $\hat{s}(\mathbf{x})$ of
Eqn.~\ref{eqn:best-s-clf}. In the common case where the density ratio is
expected to smoothly vary around $\mathbf{x}$, we would however recommend
learning models whose output value $\hat{s}(\mathbf{x})$ also smoothly varies
around $\mathbf{x}$, such as neural networks. For small training sets, tree-based
methods are not expected to work so well for this use case, since they usually
model $\hat{s}(\mathbf{x})$ as a non-strictly monotonic composition of step functions.
In particular, in such cases where $s(\mathbf{x})$ is not monotonic with $r(\mathbf{x})$,
the induced probability does not factorize as in Eqn.~\ref{eqn:factorization}, resulting in a biased
approximation of the density ratio.

Given a reduction map $s$, our results show that a statistic equivalent to the
likelihood ratio can be constructed, provided $p(s(\mathbf{x})|\theta)$ can be
evaluated. Again, we do not have a direct and exact way for evaluating this
density, but an approximation $\hat p(\hat s(\mathbf{x})| \theta)$ can be built
instead, e.g. using density estimation or calibration algorithms, such as
histograms, KDE or isotonic regression, and as applied on $\{\hat
s(\mathbf{x}_i)\}$, for samples $\{\mathbf{x}_i\}$  drawn from the generative
model. Most notably, learning such an approximation of $p(s(\mathbf{x})|\theta)$
is a much simpler problem than learning $p(\mathbf{x}|\theta)$, since the
reduction $s$ projects $\mathbf{x}$ into a one-dimensional space in which only
the (simpler) informative content of $r(\mathbf{x})$ is preserved.

One strength of the proposed approach is that it factorizes the approximation of
the per-sample reduction ($\hat{s}(\mathbf{x}) \approx s(\mathbf{x})$) from the
calibration procedure ($\hat p(\hat s(\mathbf{x})| \theta) \approx
p(\hat{s}(\mathbf{x})|\theta)$). Thus, even if the classifier does a poor job at
learning the optimal decision function~\ref{eqn:best-s-clf} and therefore at
reproducing the level sets of the per-sample likelihood ratio, the density of
$\hat{s}$ can still be well calibrated. In that case, one might loose power, but
the resulting inference will still be valid. This point was made by
\cite{Neal:2007zz} and is well appreciated by the particle physics community
that typically takes a conservative attitude towards the use of machine learning
classifiers precisely due to concerns about the calibration of $p$-values in the
face of nuisance parameters associated to the simulator.

% \subsection{Classification and frequentist hypothesis tests}
%
% \glnote{Shall we keep everything from this?}
%
% Vast literature exists around generative and discriminative
% classifiers~\citep{AndrewY.Ng}. Typically, generative classifiers learn a model
% for the joint probability $p(\mathbf{x}, y)$, of the inputs $x$ and the classification
% label $y$, and predict $p(y|\mathbf{x})$ via Bayes rule. In contrast, discriminative
% classifiers model the posterior $p(y|\mathbf{x})$ directly. For classification tasks, one
% then thresholds on $p(y|\mathbf{x})$. In both cases this description in terms of a
% posterior requires a prior distribution for $p(y)$, which is either modeled
% explicitly or learned from the training data. This familiar formulation of
% classification may lead to some confusion in the setting of the current work.
%
% The first possible source of confusion we wish to avoid is that here
% $p(\mathbf{x}|\theta)$  is a  \textit{generative statistical model} for the features $\mathbf{x}$,
% not a generative classifier. We think of the  $p(x|\theta)$ along the lines of a
% traditional scientific theory or simulator, able to make predictions about $\mathbf{x}$
% and being motivated by domain-specific considerations.
%
% The second possible source of confusion is that we are not directly interested
% in calibrating the classification score in terms of a per-event posterior
% probability $p(y|\mathbf{x})$. Instead, we are interested in the approximation of the
% per-experiment likelihood function or likelihood ratio, which might be used for
% several purposes, including the calculation of p-values.
%
% Lastly, in the setting of frequentist hypothesis tests and confidence intervals,
% we do not have a prior $\pi(\theta)$. While we can use the generative models to
% produce training data $(\mathbf{x} ,u=0)$ generated from
% $p(\mathbf{x}|\theta_0)$ and $(\mathbf{x}, u=1)$ generated from
% $p(\mathbf{x}|\theta_1)$, the relative mix $p(y)$ is arbitrary. Since the prior
% $p(y)$ is not needed for the target likelihood ratio test and because the
% classifier score $p(y|\mathbf{x})$ may not be well calibrated, we choose to denote the
% classifier score $s(\mathbf{x})$ and simply think of it as a deterministic dimensionality
% reduction map $s: \mathbb{R}^p \to \mathbb{R}$.  Similar points have been made
% by~\cite{ClaytonScott} and \cite{Neal:2007zz}.


% Generalized likelihood ratio tests ===========================================

% - mathematical problem (was section 4)
% - maximum likelihood estimator inference from ratios
% - discussion on parameterized classifier and practicalities regarding embedding (was sections 5.2 and 5.3)

\section{Generalized likelihood ratio tests}
\label{sec:generalized-likelihood-ratio}

Thus far we have shown that the target likelihood ratio
$r(\mathbf{x};\theta_0,\theta_1)=\frac{p(\mathbf{x}|\theta_0)}{p(\mathbf{x}|\theta_1)}$
with high dimensional features $\mathbf{x}$ can be reproduced via the univariate
densities $p(s(\mathbf{x})|\theta_0)$ and $p(s(\mathbf{x})|\theta_1)$ if the
reduction $s(\mathbf{x})$ is monotonic with $r(\mathbf{x};\theta_0,\theta_1)$.
We now generalize from the ratio of two simple hypotheses specified by
$\theta_0$ and $\theta_1$ to the case of composite hypothesis testing where
$\theta$ are continuous model parameters.

\subsection{Composite hypothesis testing}

% Thus far we have shown that the target likelihood ratio
% $p(x|\theta_0)/p(x|\theta_1)$ with high dimensional features $x$ can be
% reproduced via the univariate densities $p(s|\theta_0)/p(s|\theta_1)$ if the
% classifier $s(x|\theta_0, \theta_1)$ is a strictly increasing function of
% $p(x|\theta_0)/p(x|\theta_1)$. We now generalize from the ratio of two simple
% hypotheses specified by $\theta_0$ and $\theta_1$ to the case where $\theta$ are
% continuous model parameters. We postpone the practicalities of training the
% classifier and estimating the density to Section~\ref{S:classifier} and continue
% in the likelihood-free setting with idealized classifiers and their densities.


%
% In the case of a fixed classifier $s(x)$ it is possible to compute $s_e=s(x_e)$
% for the observed data and never refer back to the original features $x_e$. In
% the parametrized setting it is not possible to pre-compute $s(x_e; \theta_0,
% \theta_1)$ since $\theta_0$ and $\theta_1$ are unknown.
%
% The critical observation is that  if we postpone the evaluation of the
% classifier to the stage of evaluating the enveloping likelihood ratio, then we
% can identify the value of the parameters  that are being compared in the
% likelihood ratio with the values used as input to $s(x;\theta_0, \theta_1)$.
%
% \begin{equation}\label{eq:embedding}
% T(D; \theta_0, \theta_1) = \prod_e \frac{p(x_e|\theta_0)}{p(x_e|\theta_1)} = \prod_e  \frac{ p (s(x_e; \theta_0, \theta_1) | \theta_0)}
% {p (s(x_e; \theta_0,  \theta_1) | \theta_1) } \; .
% \end{equation}
%
% This is equivalent to approximating the likelihood function for $\theta_0$  when
% $\theta_1$ is held fixed.

% \section{Composite hypotheses and the generalized likelihood ratio}\label{S:GLR}

In the case of composite hypotheses $\theta \in \Theta_0$ against an alternative
$\theta \in \Theta_1$ (such that $\Theta_0 \cap \Theta_1 = \emptyset$ and $\Theta_0 \cup \Theta_1 = \Theta$), the
generalized likelihood ratio test, also known as the profile likelihood ratio
test, is commonly used
\begin{equation}\label{eqn:generalized-lr}
\Lambda({\cal D}; \Theta_0, \Theta) =  \frac{ \sup_{\theta \in \Theta_0} p({\cal D} | \theta)}{ \sup_{\theta \in \Theta} p({\cal D} | \theta)} \; .
\end{equation}
This generalized likelihood ratio can be used both for hypothesis tests in the
presence of nuisance parameters or to create confidence intervals with or
without nuisance parameters.  Often, the parameter vector is broken into two
components $\theta=(\mu,\nu)$, where the $\mu$ components are considered
parameters of interest while the $\nu$ components are considered nuisance
parameters. In that case $\Theta_0$ corresponds to all values of $\nu$ with
$\mu$ fixed.

Evaluating the generalized likelihood ratio as defined by
Eqn.~\ref{eqn:generalized-lr} requires finding for both the numerator and the
denominator the maximum likelihood estimator
\begin{equation}\label{eq:mle}
    \hat{\theta} = \argmax_\theta p({\cal D} | \theta).
\end{equation}
Again, this is made difficult in the likelihood-free setting and it is not
obvious that we can find the same estimators if we are working instead with
$p(s(\mathbf{x})|\theta)$. Fortunately, there is a construction
based on $s$ that works: the maximum likelihood estimate of Eqn.~\ref{eq:mle} is
the same as the value that maximizes the likelihood ratio with respect to
$p({\cal D}|\theta_1)$, for some fixed value of $\theta_1$ chosen such that the support of $p(\mathbf{x}|\theta_1)$ covers the support of $p(\mathbf{x}|\theta)$.
This allows us to
use Theorem~\ref{thm:ratio-equivalence} to reformulate the maximum likelihood
estimate as
\begin{align}\label{eq:mle_withs}
\hat{\theta} &= \argmax_\theta  p({\cal D} | \theta) \nonumber \\
&= \argmax_\theta  \prod_{\mathbf{x} \in {\cal D}} \frac{p(\mathbf{x}| \theta)}{p(\mathbf{x}|\theta_1)} \nonumber \\
&= \argmax_\theta  \prod_{\mathbf{x} \in {\cal D}} \frac{p(s(\mathbf{x}; \theta, \theta_1) | \theta)}{p(s(\mathbf{x}; \theta, \theta_1) |\theta_1)} \;,
\end{align}
where $s(\mathbf{x};\theta,\theta_1)$ denotes a \textit{parameterized}
transformation $s$ of $\mathbf{X}$ in terms of $(\theta,\theta_1)$ that is monotonic
with $r(\mathbf{x};\theta,\theta_1)$. Note that it is important that we include
the denominator $p(s(\mathbf{x}; \theta, \theta_1) |\theta_1)$ because this
cancels Jacobian factors that vary with $\theta$.

Finally, once the maximum likelihood estimates have been found for both the numerator
and denominator of Eqn.~\ref{eqn:generalized-lr}, the generalized likelihood
ratio can be estimated as outlined in Section~\ref{sec:approx}
for simple hypothesis testing.

% For completeness, the proposed method is summarized in
% Algorithm~\ref{alg:training}, while further details for training and calibrating
% efficiently a family $s(\mathbf{x};\theta_0,\theta_1)$ of classifiers are
% discussed in sections \ref{sec:param-clf} and \ref{sec:param-calibration}.


\subsection{Parameterized classification}
\label{sec:param-clf}

In order to provide parameter inference in the likelihood-free setting as
described above, we must train a family $s(\mathbf{x};\theta_0,\theta_1)$ of
classifiers parameterized by $\theta_0$ and $\theta_1$, the parameters
associated to the null and alternate hypotheses, respectively. While this could
be done independently for all $\theta_0$ and $\theta_1$, using the procedure
outlined in Section~\ref{sec:likelihood-ratio-tests}, it is desirable and
convenient to have a smooth evolution of the classification score as a function
of the parameters. For this reason, we anticipate a single learning stage based
on training data with input $(\mathbf{x}, \theta_0, \theta_1)_i$ and target
$y_i$, as outlined in Algorithm~\ref{alg:training}.
Somewhat unusually, the
unknown values of the parameters are taken as input to the classifier; their
values will be specified via the enveloping (generalized) likelihood ratio of
Eqn.~\ref{eqn:generalized-lr}.  In this way, the parameterized classifier
now models the distribution of the output $y$ conditional to $(\mathbf{x}, \theta_0, \theta_1)$,
for any $\mathbf{x}$ and any combination of parameter values $\theta_0, \theta_1$.

While the optimal decision function \ref{eqn:best-s-clf} is expected to be
learned for the parameter values $\theta_0$ and $\theta_1$ selected in
Algorithm~\ref{alg:training}, it is not clear whether the optimal decision
function can be expected for data generated from  $\theta'_0$ and $\theta'_1$
never jointly encountered at learning. Similarly, it is not clear how the
limited capacity of the classifier may impact the performance the resulting
parameterized decision function. These issues are left as an area for future
work, but preliminary exploration by \cite{Baldi:2016fzo} show that
Algorithm~\ref{alg:training} is an effective practical approach.

\begin{algorithm}[t]
\caption{Learning a parameterized classifier.}\label{alg:training}
\begin{algorithmic}
    \State ${\cal L := \{ \}}$;
    \While{ $\text{size}({\cal L}) < N $ }
        \State Select or draw $\theta_0$, $\theta_1$ from $\Theta_0$, $\Theta_1$;
	    \State Draw $\mathbf{x} \sim p(\mathbf{x}|\theta_0)$;
		\State ${\cal L} := {\cal L} \cup \{ ((\mathbf{x}, \theta_0, \theta_1), y=0) \}$;
		\State Draw $\mathbf{x} \sim p(\mathbf{x}|\theta_1)$;
		\State ${\cal L} := {\cal L} \cup \{ ((\mathbf{x}, \theta_0, \theta_1), y=1) \}$;
    \EndWhile
    \State Learn a single classifier $s(\mathbf{x}; \theta_0, \theta_1)$ from ${\cal L}$.
\end{algorithmic}
\end{algorithm}


\subsection{Parameterized calibration}
\label{sec:param-calibration}

Once the classifier is trained, we can use the generative model together with a
univariate density estimation technique (e.g. histograms or kernel density
estimation) to approximate $p(s|\theta)$ for specific parameter
points. For a single parameter point $\theta$, this is a tractable univariate density
estimation problem. The challenge comes from the need to calibrate this density
for all values of $\theta$. A straightforward approach would be to run the
generative model on demand for any particular value of $\theta$. In the context
of a likelihood fit this would mean that the optimization algorithm that is
trying to maximize the likelihood with respect to $\theta$ needs access to the
generative model $p(\mathbf{x}|\theta)$. This can be  impractical when the generative
model is computationally expensive or has high-latency (for instance some human
intervention is required to reconfigure the generative model).

In high energy physics, with a
fixed classifier, it has become common  to interpolate the distribution between
discrete values of $\theta$ in order to produce a continuous parameterization for
$p(s | \theta)$~\citep{read1999linear,Cranmer:2012sba,baak2015interpolation}.
One can easily imagine a number of approaches to embedding the classifier and
estimating the density $p(s|\theta)$ and the relative merits of those
approaches will depend critically on the dimensionality of $\theta$ and the
computational cost of the generative model. We leave a more general strategy for
this overarching optimization problem as an area of future work.

% There is a mild technical challenge in embedding the classifier into the
% likelihood. In the case of a fixed classifier $\hat s(x)$ it is possible to
% pre-compute $\hat s_e=\hat s(x_e)$ and never refer back to the original features
% $x_e$. In the parametrized setting it is not possible to pre-compute $\hat
% s(x_e; \theta_0, \theta_1)$ since one does not know the true values of
% $\theta_0$ and $\theta_1$. Thus one must implement the embedding of the
% classifier as in Eq.~\ref{eq:embedding}.
%  A concrete realization of this has
% been performed for probability models implemented with the \texttt{RooFit}
% probabilistic programing language and  classifiers implemented with
% \texttt{scikit-learn} and
% \texttt{TMVA}~\citep{Verkerke:2003ir,scikit-learn,Hocker:2007ht}.


% Applications =================================================================

% - typical usage in HEP with a concrete experimental example.
%   * illustrate the method for several classifiers (new)
%    * illustrate the method for several calibration routines (new)
%    * compare with other methods (new)
% - ratio of mixtures and decomposition (special case of generalized likelihood rario tests when parameters only affect the mixture coefficients => no need for parametrized classifiers)
%   discuss about focusing the capacity of the classifier
% - measure of particles properties, with a concrete example. Comparison with other likelihood-free inference methods (new)


\subsection{Mixture models}
\label{sec:mixture}

% In this section we generalize the capacity focusing technique of training
% classifiers to discriminate between components of a mixture model. First, we
% generalize Eq.~\ref{eq:hepGen} to a mixture model of several components
% \begin{equation}
% p(x|\theta)=\sum_c w_c(\theta) p_c(x| \theta) \;.
% \end{equation}
% It is possible to re-write the target likelihood ratio between two mixture models in terms of pairwise classification problems.
% \begin{align}
% \frac{p(x|\theta_0)}{p(x|\theta_1)} &= \frac{\sum_c w_c(\theta_0) p_c(x| \theta_0)}{\sum_{c'} w_{c'}(\theta_1) p_{c'}(x| \theta_1)} \nonumber \\
% &= \sum_c \left[ \sum_{c'} \frac{ w_{c'}(\theta_1)}{w_c(\theta_0)} \frac{ p_{c'}(x| \theta_1)}{  p_c(x| \theta_0)}  \right]^{-1} \nonumber \\
% &= \sum_c \left[ \sum_{c'} \frac{ w_{c'}(\theta_1)}{w_c(\theta_0)} \frac{ p_{c'}(s_{c,c',\theta_0, \theta_1}| \theta_1)}{ p_c(s_{c,c',\theta_0, \theta_1}| \theta_0)}  \right]^{-1} \label{eq:decomposedResult}
% \end{align}
% The second line is a trivial, but useful decomposition into pair-wise classification between $p_{c'}(x|\theta_1)$ and $p_c(x|\theta_0)$.  The third line uses Theorem~1 to relate the high-dimensional likelihood ratio into an equivalent calibrated likelihood ratio based on the univariate density of the corresponding classifier, denoted $s_{c,c',\theta_0, \theta_1}$. In the situation where the only free parameters of the  model are the mixture coefficients $w_c$, then the distributions $p_{c}(s_{c,c',\theta_0, \theta_1}| \theta)$ are independent of $\theta$ and can be pre-computed (after training the discriminative classifier, but before evaluating the  likelihood ratio). Equation~\ref{eq:decomposedResult} allows one to take advantage of both the parameterized classifier as in Eq.~\ref{eq:hep_improved} and the capacity focusing technique in the typical HEP usage pattern.

In the special case of (simple or composite) hypothesis testing between
models defined as mixtures of several components, i.e. when $p(\mathbf{x}|\theta)$ can be written as
\begin{equation}
p(\mathbf{x}|\theta)=\sum_c w_c(\theta) p_c(\mathbf{x}| \theta),
\end{equation}
the target likelihood ratio can be formulated in terms of pairwise
classification problems. We indeed have
\begin{align}
\frac{p(\mathbf{x}|\theta_0)}{p(\mathbf{x}|\theta_1)} &= \frac{\sum_c w_c(\theta_0) p_c(\mathbf{x}| \theta_0)}{\sum_{c'} w_{c'}(\theta_1) p_{c'}(\mathbf{x}| \theta_1)} \nonumber \\
&= \sum_c \left[ \sum_{c'} \frac{ w_{c'}(\theta_1)}{w_c(\theta_0)} \frac{ p_{c'}(\mathbf{x}| \theta_1)}{  p_c(\mathbf{x}| \theta_0)}  \right]^{-1} \nonumber \\
&= \sum_c \left[ \sum_{c'} \frac{ w_{c'}(\theta_1)}{w_c(\theta_0)} \frac{ p_{c'}(s_{c,c'}(\mathbf{x};\theta_0, \theta_1)| \theta_1)}{ p_c(s_{c,c'}(\mathbf{x};\theta_0, \theta_1)| \theta_0)}  \right]^{-1}. \label{eq:decomposedResult}
\end{align}
The second line is a trivial, but a useful decomposition into pairwise
density ratio sub-problems between $p_{c'}(\mathbf{x}|\theta_1)$ and
$p_c(\mathbf{x}|\theta_0)$.  The third line uses
Theorem~\ref{thm:ratio-equivalence} to relate the high-dimensional likelihood
ratio into an equivalent calibrated likelihood ratio based on the univariate
density of the corresponding classifier.

In applications where mixture models are commonly used this decomposition allows
one to construct better likelihood ratio estimates since it forces classifiers
$s_{c,c'}$ to focus on simpler sub-problems, for which higher accuracy is
expected.

Finally, as a technical point, in the situation where the only free parameters
of the  model are the mixture coefficients $w_c$, the distributions
$p_{c}(s_{c,c'}(\mathbf{x};\theta_0, \theta_1)| \theta)$ are independent of
$\theta$. For this reason, sub-ratios $\frac{
p_{c'}(s_{c,c'}(\mathbf{x};\theta_0, \theta_1)|\theta_1)}{
p_c(s_{c,c'}(\mathbf{x};\theta_0, \theta_1)|\theta_0)}$ simplify to $\frac{
p_{c'}(s_{c,c'}(\mathbf{x}))}{ p_c(s_{c,c'}(\mathbf{x}))}$, and can all be
pre-computed without having to resort to parameterized classification and
calibration.



\section{Examples and applications}
\label{sec:examples}

\subsection{Toy example}

As a simple but illustrative example, let us first consider the approximation of
the likelihood log-ratio $\log \left( r(\mathbf{x};\theta_0,\theta_1) \right)$ between the mixtures
$p(\mathbf{x}|\theta_0)$ and $p(\mathbf{x}|\theta_1)$ illustrated in
Figure~\ref{fig:1} and defined as
\begin{align}
p(\mathbf{x}|\theta_0) &= (\frac{1}{2} - g) p_{c_0}(\mathbf{x}) + (\frac{1}{2} - g) p_{c_1}(\mathbf{x}) + g p_{c_2}(\mathbf{x}), \\
p(\mathbf{x}|\theta_1) &= \frac{1}{2} p_{c_0}(\mathbf{x}) + \frac{1}{2} p_{c_1}(\mathbf{x}),
\end{align}
where $p_{c_0} := {\cal N}(\mu=-2, \sigma^2=0.5625)$, $p_{c_1} := {\cal N}(\mu=0, \sigma^2=4)$,
$p_{c_2} := {\cal N}(\mu=1, \sigma^2=0.25)$ and where $g$ is a parameter of $p(\mathbf{x}|\theta_0)$
set to $0.05$. Samples drawn from $p(\mathbf{x}|\theta_0)$ are shown in the
figure and will be used later for inference.

Using the method outlined in Section~\ref{sec:approx},
figures~\ref{fig:2a}, \ref{fig:2b} and \ref{fig:2c} show the approximated
ratio when using respectively a linear model, a 2-layer neural network or a random
forest. The blue curve corresponds to the true target log-ratio, the green curve is
the direct approximation $\log \left( \frac{\hat s(\mathbf{x})}{1 - \hat s(\mathbf{x})} \right)$
without calibration, while the red curve is the approximation $\log \frac{\hat p(\hat
s(\mathbf{x}) | \theta_0)}{\hat p(\hat s(\mathbf{x}) | \theta_1)}$ as calibrated
using histograms. Finally, the
cyan curve shows the approximated ratio when decomposing the mixtures, as
described in Section~\ref{sec:mixture}.
Hyper-parameters of the classifiers are all tuned on a finite grid of values
in order to minimize the mean squared error, as estimated by 3-fold cross-validation.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/fig1.pdf}
        \caption{$p(\mathbf{x}|\theta_0)$ and $p(\mathbf{x}|\theta_1)$}
        \label{fig:1}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/fig2-a.pdf}
        \caption{Linear model}
        \label{fig:2a}
    \end{subfigure}

    \vspace{1em}

    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/fig2-b.pdf}
        \caption{Neural network}
        \label{fig:2b}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/fig2-c.pdf}
        \caption{Random forest}
        \label{fig:2c}
    \end{subfigure}
    \caption{Approximation of the log-likelihood ratio
             $\log r(\mathbf{x};\theta_0,\theta_1) = \log \frac{p(\mathbf{x}|\theta_0)}{p(\mathbf{x}|\theta_1)}$
             with calibrated classifiers.}
\end{figure}

The first fact to highlight is that, in the finite sample setting, uncalibrated
direct approximations $\log \left( \frac{\hat s(\mathbf{x})}{1 - \hat
s(\mathbf{x})} \right)$, as shown in green, do not yield very accurate estimates
of the ratio, even when using universal approximators such as a neural network
or a random forest. This observation is at the origin of this work, and led us
to show that the ratio $r(\mathbf{x})$ can still be computed exactly by forming
the equivalent statistic
$\frac{p(s(\mathbf{x})|\theta_0)}{p(s(\mathbf{x})|\theta_1)}$, provided the
transformation $s(\mathbf{x})$ is monotonic with $r(\mathbf{x})$. Indeed, when
constructing $\frac{\hat p(\hat s(\mathbf{x})|\theta_0)}{\hat p(\hat
s(\mathbf{x})|\theta_1)}$ as we propose, estimates appear much closer to the
true ratio than their direct counterparts, as illustrated in red. The
approximated ratios show to be accurate \glnote{Would be nice to be quantitative
here.} for all three classifiers, in particular in regions with enough density
(i.e., from $\mathbf{x}=-3$ to $\mathbf{x}=2$) and where the capacity of the
classifier is focused. By contrast, in regions where the densities are low
(i.e., at the boundaries) the approximated ratio show to have high variance
since the underlying classifier cannot make very accurate predictions, because
of the few training samples it is trained on in these parts of the input space.
Accordingly, when leveraging the fact that densities are mixtures and decomposing their
ratio, the capacity of the underlying classifiers can be focused on easier
supervised learning tasks, resulting as expected in even more accurate approximations, as
shown in cyan for all three classifiers.

As shown in the figures, qualitatively different results are obtained depending
the classification algorithm embedded within the approximation of the ratio. In
this case, the neural network-based approximation appears to work best, due to
the capacity of the classifier to model a decision function $s(\mathbf{x})$
continuous and almost monotonic with the ratio. By contrast, the random forest
yields a decision function which is piece-wise constant, therefore non-continuous
and not strictly monotonic with the ratio in the finite sample regime, which
appears to result in more variance in the approximated ratio. Finally, the
linear model produces a decision function that is continuous but only piece-wise
monotonic with the ratio. Due to the symmetry of the ratio around
$\mathbf{x}=1$, this still results in an accurate approximation, but poorer
results should be expected on more complex problems. Similarly, calibrating
$\hat s(\mathbf{x})$ to construct $\hat p(\hat s(\mathbf{x}))$ is also key to
obtain accurate results. While standard histograms with fixed bins have been
used here for illustrative purposes, we anticipate that the embedded density
estimation algorithm should be carefully chosen and tuned, on a case by case
basis.

Let us now consider the composite hypothesis testing setting, by comparing
$H_0: g = 0$ versus the alternative $H_1: g > 0$. That is, we want to evaluate
\begin{equation}
\Lambda({\cal D}; \Theta_0 = \{0\}, \Theta=(0, \frac{1}{2}]) =  \frac{ p({\cal D} | g = 0) }{ \sup_{\theta \in \Theta} p({\cal D} | g=\theta)} \; .
\end{equation}
Following the approach outlined in Section~\ref{sec:generalized-likelihood-ratio},
computing the generalized likelihood ratio statistic requires finding
the maximum likelihood estimator for the denominator, which can be carried out using
approximated likelihood ratios. Reusing the decomposed neural network-based
approximation (or equivalently, using a parameterized classifier) and minimizing the mean negative log-ratio of some observed data
generated for $g=0.05$ (as illustrated in Figure~\ref{fig:1}),
we successfully find as estimator $\hat g=0.049$, as shown in Figure~\ref{fig:3}.
From there, the generalized likelihood ratio can be approximated as in the simple
hypothesis setting.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/fig3.pdf}
    \caption{Using approximated likelihood ratios for parameter inference.}
    \label{fig:3}
\end{figure}

\glnote{Smoothly interpolate between histogram bins, in order to get a smooth ratio as well?}
\glnote{Discuss diagnostics?}
\glnote{Better highlight advantages of the method?}
\glnote{Conduct more thorough and convincing experiments?}

\subsection{High energy physics}

In high energy physics, we are often searching for some class of events,
generically referred to as \textit{signal}, in the presence of a separate class
of \textit{background} events.  For each event we measure some quantities
$\mathbf{x}$, with corresponding distributions $p_s(\mathbf{x}|\nu)$ for signal
and $p_b(\mathbf{x}|\nu)$ for background, and where $\nu$ are nuisance
parameters describing uncertainties in the underlying physics prediction or
response of the measurement device. The total model is a mixture of the signal
and background, and $\mu$ is the mixture coefficient associated to the signal
component, that is
\begin{equation}\label{eq:hepGen}
p( {\cal D} | \mu, \nu) = \prod_{\mathbf{x} \in {\cal D}} \left[ \mu p_s( \mathbf{x} |  \nu)  + (1-\mu) p_b( \mathbf{x} | \nu) \right] \;.
\end{equation}
Accordingly, new particle searches at the LHC are typically framed as
hypothesis tests where the null corresponds to $\mu=0$, and the generalized
likelihood ratio is used as a test statistic.

In this setting, a classifier is usually built from a large sample of data
$\{\mathbf{x}_i, y_i\}$ generated with some nominal values of the parameters
$\nu=\nu_0$, with $y=0$ for samples drawn from the background component and
$y=1$ for samples drawn from the signal component. Importantly, the $y=1$ label
corresponds to the signal only hypothesis, and not to the alternate
signal-plus-background hypothesis. The resulting classifier approximates the
regression function
$\frac{p_s(\mathbf{x}|\nu_0)}{p_s(\mathbf{x}|\nu_0)+p_b(\mathbf{x}|\nu_0)}$, which is
one to one with the likelihood ratio of the null to the alternate
$\frac{p(\mathbf{x}|\mu=0,\nu_0)}{p(\mathbf{x}|\mu,\nu_0)}$ for all $\mu$ and therefore
satisfies conditions of Theorem~\ref{thm:ratio-equivalence}. Associating the
$y=1$ label to the signal component has advantages because it helps the
classifier focus its capacity on the relevant regions in the feature space,
particularly when the signal is a very small perturbation to the background
(i.e. $\mu \ll 1$). Once the classifier is trained, large samples of data are
generated from $p_s(\mathbf{x} | \nu)$ and $p_b(\mathbf{x} | \nu)$ and we
estimate the distributions $\hat{p}_s(\hat s(\mathbf{x}) | \nu)$ and
$\hat{p}_b(\hat s(\mathbf{x}) | \nu)$ continuously parameterized in $\nu$.

 % An example of the distributions of $\hat s$ for the
 % signal and background events with $\nu=\nu_0$ is shown in
 % Figure~\ref{fig:tmva}.

% \begin{figure}[t]
% \begin{center}
%  \includegraphics[height=1.7in]{figures/example-TMVA-BDT.pdf}
%  \includegraphics[height=1.7in]{figures/example-TMVA-ROC.pdf}
% \caption{Left: an example of the distributions $p_b(\hat s|\nu)$ and $p_s(\hat s|\nu)$ when the classifier $s$ is a boosted-decision tree (BDT). Right: the corresponding ROC curve (right) for this and other classifiers. (Figures taken from TMVA manual.)}
% \label{fig:tmva}
% \end{center}
% \end{figure}

% These steps feed into a subsequent statistical test based on the observed data
% ${D=(x_1, \dots, x_n)}$. For each event, the classifier is evaluated and one
% performs inference on a parameter $\mu$ related to the presence of the signal
% contribution. In particular, one forms the statistical model.
% % \footnote{Sometimes
% % there is an additional Poisson term when expected number of signal and
% % background events is known, which is referred to as an extended likelihood or
% % marked Poisson model.}
% \begin{equation}\label{eq:typicalML}
% p( D \,|\, \mu, \nu) = \prod_{e=1}^n \, \left[\, \mu \hat{p}_s( \hat s(x_e) \, |\,  \nu)  + (1-\mu)\, \hat{p}_b( \hat s(x_e) \,|\, \nu) \,\right] \; .
% \end{equation}

% \subsubsection{Comments on typical usage of machine learning in HEP}

Nuisance parameters are an after thought in the typical usage of machine
learning in high energy physics. In fact, most discussions related to the training and
optimizing the classifier only consider $p_b(\mathbf{x})$ and $p_s(\mathbf{x})$, with $\nu=\nu_0$
being implicit. However, as experimentalists we know that we must account for
various forms of systematic uncertainty, parameterized by nuisance parameters
$\nu$. In practice, we take the classifier $\hat s$ as fixed and then propagate
uncertainty by estimating $\hat{p}_s(\hat s(\mathbf{x}) | \nu)$. Building such
distributions for values of $\nu$ other than the nominal $\nu_0$
used at training can be thought of as a calibration necessary for
classical statistical inference. However, this classifier is clearly not optimal
for $\nu \ne \nu_0$. For the this reason, it is expected that the integrated parameterized approach proposed in this work
would yield more accurate estimates of the generalized likelihood ratio.


% \subsubsection{A more powerful  approach}
%
% The standard use of machine learning in HEP can be improved by training a
% parameterized classifier corresponding to the generalized likelihood ratio test
% \begin{equation}
% \lambda(\mu) = \frac{p(D|\mu, \hat{\hat{\nu}})}{p(D|\hat \mu, {\hat{\nu}})} \;,
% \end{equation}
% following the approach outlined in Section~\ref{S:GLR}.
%
% There is an interesting distinction between this approach and the standard use
% in which the classifier is trained for a fixed $\nu_0$. In the standard use one
% trains a classifier for signal vs. background, which is equivalent (in an ideal
% setting) to training a classifier for  null (background-only) vs. alternate
% (signal-plus-background) since the resulting regression functions are one-to-one
% with each other. In contrast, in the case of the generalized likelihood ratio
% test
% \begin{equation}\label{eq:hep_improved}
%  \frac{p(x| 0, \hat{\hat{ \nu}})}{p(x|\hat \mu, \hat\nu)} =  \frac{p_b(x| \hat{\hat{ \nu}})}{ \hat \mu p_s( x_e \, |\,  \hat\nu)  + (1- \hat \mu )\, p_b( x_e \,|\, \hat \nu)} \; ,
% \end{equation}
% the background components don't cancel and there is an additional term $p_b(x|
% \hat{\hat{ \nu}})/p_b(x| {\hat{ \nu}})$. In practice, with classifiers of finite
% capacity, there will be some trade-off between taking into account this
% additional term and the more challenging learning problem when $\mu$ is very
% small.

While the original motivation for this work was to improve the treatment of
systematic uncertainties in new particle searches by parameterizing the
classifier in terms of the nuisance parameters $\nu$, the same approach can be
used for parameter inference. In the case of new particle searches, the parameter
of interest is the mixture coefficient for the signal component $p_s(x|\nu)$.
When measuring particle properties, the distribution of the features also depend
on parameters such as a particle's mass and quantum numbers. This is easily
accommodated by extending $p_s(\mathbf{x}|\nu) \to p_s(\mathbf{x}|\theta)$, where $\theta$
includes both parameters of interest and nuisance parameters.
This formalism represents a significant step forward in the usage of machine
learning in high energy physics, where classifiers have always been used between two static
classes of events and not parameterized explicitly in terms of the physical
quantities we wish to measure.
The work of \cite{Whiteson:2006ws} is similar,
as the stochastic optimization was directly trying to minimize the measurement
uncertainty of a particle's mass; however, the resulting classifier was fixed.
This approach also offers the advantage that it explicitly reformulates the
per-experiment optimization to the per-event optimization, which is less
computationally intensive.

Another approach that is similar in spirit is the so-called matrix element
method, in which one  directly computes an approximate likelihood ratio by
performing a computationally intensive integral associated to the detector
response~\citep{Volobouev:2011vb}. In the approach considered in this paper, the
detector response is naturally handled by the Monte Carlo sampling used in the
simulation of the detector; however, that integral is intractable for the matrix
element method. Even with drastic simplifications of the detector response, the
matrix element method can take several minutes of CPU time to calculate the
likelihood ratio for a single event. The work here can be seen as aiming
at the same conceptual target, but relying on machine learning to overcome the
complexity of the detector simulation. It also offers enormous speed increase
for evaluating the likelihood at the cost of an initial training stage. In
practice, the matrix element method has only been used for searches and
measurement of a single physical parameter (sometimes with a single nuisance
parameter as in~\citep{Aaltonen:2010yz}).

Contemporary examples where the technique presented here could have major impact
include the measurement of coefficients to quantum mechanical operators
describing the decay of the Higgs boson~\citep{Chen:2014pia} and, if we are so
lucky, measurement of the mass of supersymmetric particles in cascade
decays~\citep{Allanach:2000kt}.  Both of these examples involve data sets with
many events, each with a feature vector $\mathbf{x}$ that has on the order of 10
components, and a parameter vector $\theta$ with 5-10 parameters of interest and
possibly many more nuisance parameters. The state of the art for the operator
coefficients of the Higgs decay uses the so-called matrix element likelihood
analysis (MELA) in which the equivalent of $s(\mathbf{x}; \theta_0, \theta_1)$ is
approximated by neglecting detector effects~\citep{Gao:2010qx,Bolognesi:2012mm}.


% Related works ================================================================

\section{Related work}
\label{sec:related}

The closest work to the proposed method is due to \cite{Neal:2007zz}, who
similarly considers the problem of approximating the likelihood function when
only a generative model is available. That work sketches a scheme in which one
uses a classifier with both $\mathbf{x}$ and $\theta$ as an input to serve as a
dimensionality reduction map. The key distinction comes in the handling of
$\theta$.  Neal argues that a classifier cannot be used on real data, since we
do not know the correct value for $\theta$, and goes on to outline an approach
where one uses regression on a per-event basis to estimate
$\hat{\theta}(\mathbf{x})$ and perform the composition $s(\mathbf{x};
\hat{\theta}(\mathbf{x}))$. As pointed out by the author, this can lead to a
significant loss of information since a single observation $\mathbf{x}$ may
carry little information about the true value of $\theta$, though a full data
set ${\cal D}$ may be informative.
% for instance, a single observation would
% not be sufficient to estimate the variance of a distribution, though repeated
% observations would.
The work of \cite{Neal:2007zz} correctly identifies this as
an approximation of the target likelihood even in the case of a ideal
classifier. In contrast, the approach described here does not eliminate the
dependence of the classifier on $\theta$.
% \footnote{As a technical point, in
% \citep{Neal:2007zz}, the focus is on approximating the likelihood function (up
% to a multiplicative constant), which is equivalent to evaluating the ratio  with
% respect to a fixed $\theta_1$ as in Eqn.~\ref{eq:mle_withs}. In
% \citep{Neal:2007zz}, the dependence on $\theta$ is eliminated via $s(\mathbf{x};
% \hat{\theta}(\mathbf{x}))$ and the map is constant; however, in this approach
% the map ratio is explicitly parameterized in terms of $\theta$, so the ratio is
% important for canceling the corresponding Jacobian factors. \glnote{Shall we
% keep this footenote? Would be better to give details in the text. JASA
% recommends not using footnotes.}}.
Instead, we embed a parameterized classifier
into the likelihood and postpone the evaluation of the classifier to the point
of evaluation of the likelihood when $\theta$ is explicitly being tested. This
avoids the loss of information that occurs from the regression step
$\hat{\theta}(\mathbf{x})$ proposed by \cite{Neal:2007zz} and leads to
Theorem~\ref{thm:ratio-equivalence}, which is an exact result in the case of an
ideal classifier. In both cases, the quality of the classifier is factorized
from the calibration of its density, which allows for valid inference even if
there is a loss of power due to a non ideal classifier.

Also close to our work, \cite{ClaytonScott} and \cite{JMLR:v14:tong13a} consider
the machine learning problem associated to Neyman-Pearson hypothesis testing. In
a similar setup, they consider the situation where one does not have access to
the underlying distributions, but only has i.i.d. samples from each hypothesis.
This work generalizes that goal from the Neyman-Pearson setting to generalized
likelihood ratio tests and emphasizes the connection with classification.
% Perhaps a  formal treatment similar to the Neyman-Pearson case can be brought to
% bear in this more general setting.
\cite{Ihler2004} take on a different problem
(tests of statistical independence) by using machine learning algorithms to find
scalar maps from the high-dimensional feature space that achieve the desired
statistical goal when the fundamental high-dimensional test is intractable.

% In a similarly titled work, \cite{Gutmann2014} advocate using
% the cross-validated classification accuracy as the similarity metric used in
% ABC. While the goal there is also parameter inference in the likelihood-free
% setting,  their method is very different than the approach presented here.
% \cite{TommiJaakkola} explore a way of leveraging generative models to derive
% kernel functions for use in discriminative methods. This interesting work is
% distinct from the point made here in which the generative model is being used
% for the purpose of providing training data and calibration.
% \cite{McCallum} consider a hybrid generative/discriminative classifier;
% however, the goal of that work is not to leverage a generative model for the
% data, but to use both approaches to learn different subsets of the parameters in
% a single hybrid classifier.
% \cite{BiancaZadrozny} emphasize the importance of calibrated probability
% estimates from decision trees and naive Bayesian classifiers and investigate
% various approaches to achieve this. In contrast to that work, we are not
% interested in calibrated probability estimates for $p(y|x)$ for individual
% events, but instead we use the calibration to correct for non-linear
% transformations of the target likelihood ratio and, perhaps, to provide
% calibrated p-values based on those likelihood ratio tests.

More generally, likelihood ratio testing directly relates to the density ratio
estimation problem, which consists in estimating the ratio of two probability
densities from given and finite collections of observations ${\cal D}_0$ and
${\cal D}_1$. Density ratio estimation is connected to many machine learning
fundamental problems, including transfer learning~\citep{sugiyama2012machine},
probabilistic classification and regression~\citep{vapnik1998statistical},
outlier detection~\citep{hido2011statistical}, and many others. For learning
under covariate shift, \cite{shimodaira2000improving} and \cite{sugiyama2005input} estimate
the density ratio $r(\mathbf{x};\theta_0,\theta_1) =
\frac{p(\mathbf{x}|\theta_0)}{p(\mathbf{x}|\theta_1)}$ from straightforward
approximations $\hat{p}(\mathbf{x}|\theta_0)$ and $\hat{p}(\mathbf{x}|\theta_1)$
separately obtained using kernel density estimation. Despite its theoretical
consistency, this approach is known to be ineffective in
practice~\citep{sugiyama2007covariate,bickel2009discriminative}, since
it relies on modeling numerator and denominator high-dimensional densities,
which is a harder problem than modeling their ratio only.
While the proposed method also proceeds in two similar steps, estimating
$p(s(\mathbf{x}))$ is much easier than estimating $p(\mathbf{x})$,
since $s$ projects $\mathbf{x}$ into a one-dimensional space in which only
the informative content of $r(\mathbf{x})$ is preserved.
Finally, in contrast with the
proposed method which decouples reduction from calibration, other
approaches proposed within the literature (see
\cite{sugiyama2012density,gretton2009covariate,nguyen2010estimating,vapnik2013constructive}
and references therein) provide solutions for estimating
$r(\mathbf{x};\theta_0,\theta_1)$ directly from $\mathbf{x}$, in one step. Under some
assumptions, the convergence of the obtained estimates is also proven for some
of these approaches.


% Conclusions ==================================================================

\section{Conclusions}
\label{sec:conclusions}

In this work, we have outlined an approach to reformulate generalized likelihood
ratio testing over a high-dimensional data set in terms of a univariate density
of a classifier score. We have shown that a parameterized family of
discriminative classifiers $\hat s(\mathbf{x}; \theta_0, \theta_1)$ trained and
calibrated with a simulator can be used to approximate the likelihood ratio,
even when it is not possible to directly evaluate the likelihood
$p(\mathbf{x}|\theta)$.
% A technique for decomposing this ratio when the generative model is a mixture
% of components was presented with the aim to help focus capacity of the
% classifier when $p(x|\theta_0)$ and $p(x|\theta_1)$ differ primarily by a small
% mixture coefficient.  This approach leverages the power of machine learning in a
% classical statistical setting.
The proposed method offers an alternative to approximate Bayesian computation
for parameter inference in the likelihood-free setting that can also be used in
the frequentist formalism without specifying a prior over the parameters. A
strength of this approach is that it separates the quality of the approximation
of the target likelihood from the quality of the calibration. The former is
related to the ability of supervised learning approaches to  classification,
which will continue to improve. The calibration procedure for a particular
parameter point is fairly straightforward since it involves estimating a
univariate density using a generative model of the data. The difficulty of the
calibration stage is performing this calibration continuously in $\theta$.
Different strategies to this calibration are anticipated depending on the
dimensionality of $\theta$, the complexity of the resulting likelihood function,
or the practical issues associated to running the simulator.

\bibliographystyle{apalike}
\bibliography{learning.bib}

\appendix

\section{Probabilistic classification for building $s$}
\label{app:clf-for-s}

In this appendix, we show for completeness that the probabilistic classification framework
yields a reduction $s$ which satisfies conditions of Theorem~\ref{thm:ratio-equivalence}.

\begin{proposition} \label{thm:best-classifier}
Let $\mathbf{X} = (X_1, ..., X_p)$ and $Y$ be random input and output variables
with values in ${\cal X} \subseteq \mathbb{R}^p$
and ${\cal Y} = \{0, 1\}$ and mixed joint probability density  function
$p_{\mathbf{X},Y}(\mathbf{x}, y)$. For the squared error loss, the best
regression function $s : {\cal X} \mapsto [0, 1]$, or equivalently the best
probabilistic classifier, is
\begin{equation}
s^*(\mathbf{x}) = \frac{P(Y=1) p_{\mathbf{X}|Y}(\mathbf{x}|Y=1)}{P(Y=0) p_{\mathbf{X}|Y}(\mathbf{x} | Y=0) + P(Y=1) p_{\mathbf{X}|Y}(\mathbf{x} | Y=1)}.
\end{equation}
\end{proposition}

\begin{proof}
For the squared error loss,
\begin{align}
s^*(\mathbf{x}) &= \argmin_{s(\mathbf{x})} \mathbb{E}_{Y|\mathbf{X}=\mathbf{x}} \{ (Y - s(\mathbf{x}))^2 \} \nonumber \\
&=  \argmin_{s(\mathbf{x})} \mathbb{E}_{Y|\mathbf{X}=\mathbf{x}} \{ Y^2 \} - 2s(\mathbf{x}) \mathbb{E}_{Y|\mathbf{X}=\mathbf{x}} \{ Y \} + s(\mathbf{x})^2 \nonumber \\
&=  \argmin_{s(\mathbf{x})} -2s(\mathbf{x}) \mathbb{E}_{Y|\mathbf{X}=\mathbf{x}} \{ Y \} + s(\mathbf{x})^2
\end{align}
The last expression is minimized when $\frac{d}{ds(\mathbf{x})} (-2s(\mathbf{x}) \mathbb{E}_{Y|\mathbf{X}=\mathbf{x}} \{ Y \} + s(\mathbf{x})^2) = 0$,
that is when $-2 \mathbb{E}_{Y|\mathbf{X}=\mathbf{x}} \{ Y \} + 2 s(\mathbf{x}) = 0$, hence
\begin{equation}
s^*(\mathbf{x}) = \mathbb{E}_{Y|\mathbf{X}=\mathbf{x}} \{ Y \}.
\end{equation}
For ${\cal Y} = \{ 0, 1 \}$,
\begin{align}
\mathbb{E}_{Y|\mathbf{X}=\mathbf{x}} \{ Y \} &= P(Y=0|\mathbf{X}=\mathbf{x}) \times 0 +  P(Y=1|\mathbf{X}=\mathbf{x}) \times 1 \nonumber \\
&= \frac{P(Y=1) p_{\mathbf{X}|Y}(\mathbf{x}|Y=1)}{p_{\mathbf{X}}(\mathbf{x})} \nonumber \\
&= \frac{P(Y=1) p_{\mathbf{X}|Y}(\mathbf{x}|Y=1)}{P(Y=0) p_{\mathbf{X}|Y}(\mathbf{x} | Y=0) + P(Y=1) p_{\mathbf{X}|Y}(\mathbf{x} | Y=1)}.
\end{align}
\end{proof}

For $P(Y=0)=P(Y=1)=\frac{1}{2}$, the best regression function $s^*$ simplifies
to
\begin{equation}
s^*(\mathbf{x}) = \frac{p_{\mathbf{X}|Y}(\mathbf{x}|Y=1)}{p_{\mathbf{X}|Y}(\mathbf{x} | Y=0) + p_{\mathbf{X}|Y}(\mathbf{x} | Y=1)}.
\end{equation}
If we further assume that samples for $Y=0$ (resp. $Y=1$) are drawn from some parameterized
distribution with probability density $p_{\mathbf{X}}(\mathbf{x}|\theta_0)$ (resp. $p_{\mathbf{X}}(\mathbf{x}|\theta_1)$), then the best regression function can be rewritten
as
\begin{equation}
s^*(\mathbf{x}) = \frac{p_{\mathbf{X}}(\mathbf{x}|\theta_1)}{p_{\mathbf{X}}(\mathbf{x} | \theta_0) + p_{\mathbf{X}}(\mathbf{x} | \theta_1)}.
\end{equation}
In particular, this regression function satisfies conditions of
Theorem~\ref{thm:ratio-equivalence} since $s^*(\mathbf{x}) =
m(\frac{p_\mathbf{X}(\mathbf{x}|\theta_0)}{p_\mathbf{X}(\mathbf{x}|\theta_1)})$,
for $m(r(\mathbf{x})) = \frac{1}{1 + r(\mathbf{x})}$, is monotonic with
$\frac{p_\mathbf{X}(\mathbf{x}|\theta_0)}{p_\mathbf{X}(\mathbf{x}|\theta_1)}$.

Proposition~\ref{thm:best-classifier} holds for the squared error loss, but it
can be similarly shown that classifiers minimizing the exponential loss, the
binomial log-likelihood (or cross-entropy) or the squared hinge loss are also
monotonic with the density ratio~\citep{friedman2000additive,lin2002support}.
However, a classifier with discrete outputs and minimizing the zero-one loss
does not satisfy conditions of the theorem.

\end{document}
